{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-free Methods\n",
    "- Previously we explored dynamic programming for finding the optimal policy for our frozenlake game\n",
    "    - In that frozenlake game, we knew our rewards and transition probability functions\n",
    "        - In the deterministic environment, whatever action we took would have a 100% probability of landing in a following state we knew\n",
    "        - In the stochastic environment, whatever action we took would have a 1/3 probability each of landing in 3 different states\n",
    "            - In the known [transition probability function example](https://www.deeplearningwizard.com/deep_learning/deep_reinforcement_learning_pytorch/dynamic_programming_frozenlake/#transition-probability-function), if we took left (action = 0) given we are in state 10, we would end up in either 9 (left of state 10), 6 (above of state 10) or 14 (below of state 10) each with 1/3 probability\n",
    "    - Because we know our transition probability function and rewards, it essentially becomes a planning problem solvable by dynamic programming\n",
    "- But in the cases where we do not have transition probability function, we need model-free methods (essentially trial-and-error methods)\n",
    "    - We will be exploring the Blackjack environment\n",
    "    - In this case, when we take or pass a card, because we do not know what card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exploring Blackjack Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observational and Action Space\n",
    "- Action space\n",
    "    - Stick: don't take a card (pass)\n",
    "        - This must be the final action, otherwise you can hit (take a card) multiple times before this action\n",
    "    - Hit: take a card\n",
    "- Observational space\n",
    "    - player's current sum of all cards\n",
    "        - $\\{0, 1, \\ldots, 31\\}$\n",
    "        - where 1 = Ace, 2-10 = Number cards, Jack/Queen/King = 10\n",
    "    - dealer's face-up card\n",
    "        - $\\{1, \\ldots, 10\\}$\n",
    "        - where 1 = Ace, 2-10 = Number cards, Jack/Queen/King = 10\n",
    "    - player's usable ace\n",
    "        - $\\{0, 1\\}$\n",
    "        - where 0 = no, 1 = yes\n",
    "        \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "# Import gym\n",
    "import gym\n",
    "\n",
    "# Make the environment\n",
    "env = gym.make('Blackjack-v0')\n",
    "\n",
    "# Tuple of (player sum of cards, dealer's face up card, player's usable ace)\n",
    "print(env.observation_space)\n",
    "\n",
    "# STICK (don't take a card) or HIT (take a card)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Game Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Game number 1\n",
      "Initial state (12, 4, False)\n",
      "Hit, action = 1\n",
      "Final state (22, 4, False)\n",
      "End game, Reward = -1\n",
      "Lost game.\n",
      "==================================================\n",
      "Game number 2\n",
      "Initial state (13, 8, False)\n",
      "Hit, action = 1\n",
      "Pass, action = 0\n",
      "Final state (18, 8, False)\n",
      "End game, Reward = 1.0\n",
      "Won game.\n"
     ]
    }
   ],
   "source": [
    "# Set environment seed for replicability\n",
    "env.seed(1337)\n",
    "env.action_space.seed(1337)\n",
    "\n",
    "# Run through 2 games\n",
    "for game_idx in range(2):\n",
    "    # Reset environment\n",
    "    state = env.reset()\n",
    "    print('='*50)\n",
    "    print('Game number {}'.format(game_idx + 1))\n",
    "    \n",
    "    # Game 1: Player sum of cards = 12, Dealer's open card = 4, Player no Ace\n",
    "    # Game 2: Player sum of cards = 13, Dealer's open card = 8, Player no Ace\n",
    "    print('Initial state {}'.format(state))\n",
    "\n",
    "    while True:\n",
    "        # Randomly sample action from the action space, still will randomly return 0 or 1\n",
    "        # action = 1 means take a card, action = 2 means pass\n",
    "        action = env.action_space.sample()\n",
    "        if action == 1:\n",
    "            print('Hit, action = {}'.format(action))\n",
    "        else:\n",
    "            print('Pass, action = {}'.format(action))\n",
    "        \n",
    "        state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            print('Final state {}'.format(state))\n",
    "            print('End game, Reward = {}'.format(reward))\n",
    "            \n",
    "            if reward > 0:\n",
    "                print('Won game.')\n",
    "            else:\n",
    "                print('Lost game.')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MC methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Policy Function\n",
    "- Sample a preset policy based on the following configuration\n",
    "    - If the player's sum of cards > 16\n",
    "        - STICK probability = 80%\n",
    "        - HIT probability = 20%\n",
    "    - If the player's sum of cards <= 16\n",
    "        - STICK probability = 20%\n",
    "        - HIT probability = 80%\n",
    "- Essentially we want the player to be inclined to take a card (HIT) when the sum of cards is not high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((14, 8, False), 1, -1)]\n",
      "[((11, 4, False), 1, 0), ((14, 4, False), 1, -1)]\n",
      "[((18, 6, False), 1, 0), ((20, 6, False), 0, 1.0)]\n",
      "[((14, 5, False), 0, -1.0)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "env.seed(100)\n",
    "np.random.seed(100)\n",
    "\n",
    "def sample_policy(env):\n",
    "    episode = []\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    \n",
    "    # Infinite loop until done\n",
    "    # only done when player STICK which is to pass to take a card (pass to HIT essentially)\n",
    "    while True:\n",
    "        # If the player's sum is more than 16, we STICK with 80% prob and HIT with 20% prob\n",
    "        if state[0] > 16:\n",
    "            action_probabilities = [0.8, 0.2]\n",
    "        # If the player's sum is less than or equal to 16, we HIT with 80% probability and STICK with 20% probability\n",
    "        else:\n",
    "            action_probabilities = [0.2, 0.8]\n",
    "        \n",
    "        # Get action based on action probabilities\n",
    "        action = np.random.choice(np.arange(2), p=action_probabilities)\n",
    "        \n",
    "        # Pass action to environment\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Append the state prior to the action for log\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    return episode\n",
    "\n",
    "\n",
    "# Loop through 2 games\n",
    "\n",
    "# First game: \n",
    "#     sum of cards <= 16, action sampled is STICK (20% probability)\n",
    "\n",
    "# Second game:\n",
    "#     sum of cards <= 16, action sampled is HIT (80% probability)\n",
    "#     sum of cards still <= 16, action sampled is STICK (20% probability)\n",
    "for i in range(4):\n",
    "    print(sample_policy(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MC Prediction Function\n",
    "- This function obtains the action-value (q-value) function estimate $Q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function defaultdicts prevents returning of key error when there is no key found when indexing\n",
    "# instead it will return the type of the argument passed to defaultdict\n",
    "# if integer function is passed, it will return 0\n",
    "# if numpy array function is passed, it will return the numpy array with 0's filled\n",
    "from collections import defaultdict\n",
    "\n",
    "# This is to enable sys.stdout.flush() for single line clean printing of status\n",
    "import sys\n",
    "\n",
    "# \n",
    "def mc_prediction_q(env, num_episodes, generate_episode, gamma=1.0):\n",
    "    # initialize empty dictionaries of arrays\n",
    "    returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    N = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            # This prevents you from printing multiple lines. Replaces existing line with new log.\n",
    "            sys.stdout.flush()\n",
    "        # generate an episode\n",
    "        episode = generate_episode(env)\n",
    "        # obtain the states, actions, and rewards\n",
    "        states, actions, rewards = zip(*episode)\n",
    "        # prepare for discounting\n",
    "        discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n",
    "        # update the sum of the returns, number of visits, and action-value \n",
    "        # function estimates for each state-action pair in the episode\n",
    "        for i, state in enumerate(states):\n",
    "            returns_sum[state][actions[i]] += sum(rewards[i:]*discounts[:-(1+i)])\n",
    "            N[state][actions[i]] += 1.0\n",
    "            Q[state][actions[i]] = returns_sum[state][actions[i]] / N[state][actions[i]]\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obtain the action-value function\n",
    "Q = mc_prediction_q(env, 500000, generate_episode_from_limit_stochastic)\n",
    "\n",
    "# obtain the corresponding state-value function\n",
    "V_to_plot = dict((k,(k[0]>18)*(np.dot([0.8, 0.2],v)) + (k[0]<=18)*(np.dot([0.2, 0.8],v))) for k, v in Q.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To use model-based methods we need to have complete knowledge of the environment\n",
    "#i.e. we need to know Pss’ (please refer to above picture): the transition probability we’ll end up in state St+1=s’ if the agent is in state St=1 and takes action At=a. For example, if a bot chooses to move forward, it might move sideways in case of slippery floor underneath it. In games like Blackjack our action space is limited like we can either choose to “hit” or “stick” but we can end up in any of the many possible states of which you know none of the probabilities! In Blackjack state is determined by your sum, the dealers sum and whether you have a usable ace or not as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500000/500000."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
