{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN from Scratch\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ritchieng/deep-learning-wizard/blob/master/docs/deep_learning/fromscratch/fromscratch_cnn.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an implementation of a simple CNN (one convolutional function, one non-linear function, one max pooling function, one affine function and one softargmax function) for a 10-class MNIST classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot Single Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (70000, 784)\n",
      "y shape: (70000,)\n",
      "Single sample shape: (28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print dataset\n",
    "print(f'X shape: {X.shape}')\n",
    "print(f'y shape: {y.shape}')\n",
    "\n",
    "# Print size of resized single sample\n",
    "single_sample = X[0, :].reshape(28, 28)\n",
    "print(f'Single sample shape: {single_sample.shape}')\n",
    "\n",
    "# Plot single sample (M x N matrix)\n",
    "plt.gray()\n",
    "plt.matshow(single_sample)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape (60000, 784)\n",
      "Testing shape (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Train-test split\n",
    "train_samples = 60000\n",
    "random_state = check_random_state(0)\n",
    "permutation = random_state.permutation(X.shape[0])\n",
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "X = X.reshape((X.shape[0], -1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_samples, test_size=10000)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Print shapes\n",
    "print(f'Training shape {X_train.shape}')\n",
    "print(f'Testing shape {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forwardpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Input shape: \t [28, 28]\n",
      "==================================================\n",
      "Padding shape: \t 0\n",
      "Output shape: \t 24\n",
      "Region shape: \t [5, 5]\n",
      "Kernel shape: \t [8, 5, 5]\n",
      "Single Slide: \t [8]\n",
      "==================================================\n",
      "Conv (f) shape: \t [24, 24, 8]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "class ConvolutionalLayer:\n",
    "    def __init__(self, num_kernels, kernel_shape):\n",
    "        # Number of kernels: 1D\n",
    "        self.num_kernels = num_kernels\n",
    "        # Shape of kernels: 2D\n",
    "        self.kernel_shape = kernel_shape\n",
    "        self.k = self.kernel_shape[0]\n",
    "        # Kernel weights: 3D\n",
    "        self.kernels_theta = torch.randn(self.num_kernels, self.kernel_shape[0], self.kernel_shape[1])\n",
    "\n",
    "    def slider(self, inp):\n",
    "        '''\n",
    "        Sliding generator that yields square areas of shape\n",
    "        (kernel_shape, kernel_shape) sliding across our input. \n",
    "        This assumes valid padding (no padding) and step size 1.\n",
    "        '''\n",
    "        h, w = inp.shape\n",
    "        for h_idx in range(h - (self.k - 1)):\n",
    "            for w_idx in range(w - (self.k - 1)):\n",
    "                single_slide_area = inp[h_idx:(h_idx + self.k), w_idx:(w_idx + self.k)]\n",
    "                yield single_slide_area, h_idx,w_idx\n",
    "\n",
    "    def forward(self, inp):\n",
    "        '''\n",
    "        Slides kernel across image doing an element-wise MM then summing.\n",
    "        Results in forward pass of convolutional layer of shape\n",
    "        (output shape, output shape, number of kernels).\n",
    "        '''\n",
    "        # Input: 2D of (height, width)\n",
    "        assert single_sample.dim() == 2, f'Input not 2D, given {single_sample.dim()}D'\n",
    "\n",
    "        # Output via Valid Padding (No Padding): 3D of (height, width, number of kernels)\n",
    "        _, w  = inp.shape\n",
    "        # P = 0\n",
    "        p = 0\n",
    "        # O = ((W - K + 2P) / S) + 1 = (28 - 3 + 0) + 1 = 25\n",
    "        o = (w - self.k) + 1\n",
    "        # Print shapes\n",
    "        print('Padding shape: \\t', p)\n",
    "        print('Output shape: \\t', o)\n",
    "        # Initialize blank tensor\n",
    "        output = torch.zeros((o, o, self.num_kernels))\n",
    "        \n",
    "        # Iterate through region\n",
    "        for single_slide_area, h_idx, w_idx in self.slider(inp):\n",
    "            if h_idx == 0 and w_idx == 0:\n",
    "                print('Region shape: \\t', list(single_slide_area.shape))\n",
    "                print('Kernel shape: \\t', list(self.kernels_theta.shape))\n",
    "                print('Single Slide: \\t', list(output[h_idx, w_idx].shape))\n",
    "                \n",
    "            # Sum values with each element-wise matrix multiplication across each kernel\n",
    "            # Instead of doing another loop of each kernel, you simply just do a element-wise MM\n",
    "            # of the single slide area with all the kernels yield, then summing the patch\n",
    "            output[h_idx, w_idx] = torch.sum(single_slide_area * self.kernels_theta, axis=(1, 2))\n",
    "            \n",
    "        # Pass through non-linearity (sigmoid): 1 / 1 + exp(-output)\n",
    "        output = 1. / (1. + torch.exp(-output))\n",
    "\n",
    "        return output\n",
    "    \n",
    "# Convert numpy array to torch tensor\n",
    "single_sample = X[0, :].reshape(28, 28)\n",
    "single_sample = torch.tensor(single_sample)\n",
    "\n",
    "print('='*50)\n",
    "print(f'Input shape: \\t {list(single_sample.shape)}')\n",
    "print('='*50)\n",
    "\n",
    "# Forward: conv\n",
    "conv = ConvolutionalLayer(num_kernels=8, kernel_shape=[5, 5])\n",
    "output = conv.forward(single_sample)\n",
    "\n",
    "print('='*50)\n",
    "print(f'Conv (f) shape: \\t {list(output.shape)}')\n",
    "print('='*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Input shape: \t [24, 24, 8]\n",
      "==================================================\n",
      "==================================================\n",
      "Pool (f) shape: \t [12, 12, 8]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "class MaxPoolLayer:\n",
    "    # O = ((W - K) / S) + 1\n",
    "    def __init__(self, pooling_kernel_shape):\n",
    "        # Assume simplicity of K = S then O = W / S\n",
    "        self.k = pooling_kernel_shape\n",
    "        \n",
    "    def slider(self, inp):\n",
    "        '''\n",
    "        Sliding generator that yields areas for max pooling.\n",
    "        '''\n",
    "        h, w, _ = inp.shape\n",
    "        output_size = int(w / self.k)  # Assume S = K\n",
    "\n",
    "        for h_idx in range(output_size):\n",
    "            for w_idx in range(output_size):\n",
    "                single_slide_area = inp[h_idx * self.k:h_idx * self.k + self.k,\n",
    "                                        w_idx * self.k:w_idx * self.k + self.k]\n",
    "                yield single_slide_area, h_idx, w_idx\n",
    "\n",
    "    def forward(self, inp):\n",
    "        '''\n",
    "        Performs a forward pass of the maxpool layer using the given input.\n",
    "        Returns a 3d numpy array with dimensions (h / 2, w / 2, num_filters).\n",
    "        - input is a 3d numpy array with dimensions (h, w, num_filters)\n",
    "        '''\n",
    "        self.last_input = inp\n",
    "\n",
    "        h, w, num_kernels = inp.shape\n",
    "        output_size = int(w / self.k)  # Assume S = K\n",
    "        output = torch.zeros(output_size, output_size, num_kernels)\n",
    "\n",
    "        for single_slide_area, h_idx, w_idx in self.slider(inp):\n",
    "            single_slide_area = torch.flatten(single_slide_area, start_dim=0, end_dim=1)\n",
    "            output[h_idx, w_idx] = torch.max(single_slide_area, dim=0).values\n",
    "\n",
    "        return output\n",
    "    \n",
    "print('='*50)\n",
    "print(f'Input shape: \\t {list(output.shape)}')\n",
    "print('='*50)\n",
    "\n",
    "# Forward: pool\n",
    "pool = MaxPoolLayer(pooling_kernel_shape=2)\n",
    "output = pool.forward(output)\n",
    "\n",
    "print('='*50)\n",
    "print(f'Pool (f) shape: \\t {list(output.shape)}')\n",
    "print('='*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine and Soft(arg)max Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Input shape: \t [12, 12, 8]\n",
      "==================================================\n",
      "Lecun initialization SD: 0.1\n",
      "input shape: \t torch.Size([1, 1152])\n",
      "weight shape: \t torch.Size([1152, 10])\n",
      "bias shape: \t torch.Size([10])\n",
      "==================================================\n",
      "Affine & Soft(arg)max (f) shape: \t [1, 10]\n",
      "==================================================\n",
      "Probas:  0.72574  0.001391  0.000083  0.202983  0.000503  0.037023  0.000428  0.000144  0.031293  0.000412\n"
     ]
    }
   ],
   "source": [
    "class AffineAndSoftmaxLayer:\n",
    "    def __init__(self, affine_weight_shape):\n",
    "        self.affine_weight_shape = affine_weight_shape\n",
    "        # Weight shape: flattened input x output shape\n",
    "        self.w = torch.zeros(self.affine_weight_shape[0] * self.affine_weight_shape[1] * self.affine_weight_shape[2], self.affine_weight_shape[3])\n",
    "        self.b = torch.zeros(self.affine_weight_shape[3])\n",
    "        \n",
    "        # Initialize weight/bias via Lecun initialization of 1 / N standard deviation\n",
    "        # Refer to DLW guide on weight initialization mathematical derivation:\n",
    "        # https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/\n",
    "        print(f'Lecun initialization SD: {1/self.affine_weight_shape[3]}')\n",
    "        self.w = torch.nn.init.normal_(self.w, mean=0, std=1/self.affine_weight_shape[3])\n",
    "        self.b = torch.nn.init.normal_(self.b, mean=0, std=1/self.affine_weight_shape[3])\n",
    "\n",
    "    def forward(self, inp):\n",
    "        '''\n",
    "        Performs Linear (Affine) Function & Soft(arg)max Function\n",
    "        that returns our vector (1D) of probabilities.\n",
    "        '''\n",
    "        inp = inp.reshape(1, -1)\n",
    "        print(f'input shape: \\t {inp.shape}')\n",
    "        print(f'weight shape: \\t {self.w.shape}')\n",
    "        print(f'bias shape: \\t {self.b.shape}')\n",
    "        logits = torch.mm(inp, self.w) + self.b\n",
    "        probas = torch.exp(logits) / torch.sum(torch.exp(logits))\n",
    "        return probas\n",
    "    \n",
    "\n",
    "print('='*50)\n",
    "print(f'Input shape: \\t {list(output.shape)}')\n",
    "print('='*50)\n",
    "\n",
    "# Forward: Affine and Softmax\n",
    "affinesoftmax = AffineAndSoftmaxLayer(affine_weight_shape=(output.shape[0], output.shape[1], output.shape[2], len(np.unique(y_train))))\n",
    "output = affinesoftmax.forward(output)\n",
    "\n",
    "print('='*50)\n",
    "print(f'Affine & Soft(arg)max (f) shape: \\t {list(output.shape)}')\n",
    "print('='*50)\n",
    "\n",
    "print(f'Probas: {pd.DataFrame(output.numpy()).to_string(index=False, header=False)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! note \"Dot Product, Matrix Multiplication, and Hadamard Product\"\n",
    "    **Hadamard product**: element-wise multiplicaton of 2 matrices.\n",
    "    </br>**Matrix Multiplication**: take the first row of the first matrix and perform dot product with each of the N columns in the second matrix to form N columns in the first row of the new matrix. Repeat for remaining rows for the first matrix.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward & GD: linear + softmax\n",
    "\n",
    "# Backward & GD: pool\n",
    "\n",
    "# Backward & GD: conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note on Contiguous vs Non-Contiguous\n",
    "- Often in the space of passing tensors, and doing all our dot products, hadamard products, matrix multiplications, transpose, reshape operations and more, you would inevitably one day encounter into an error that says your tensor is `not contiguous`. \n",
    "- This is a memory allocation problem. \n",
    "- Certain tensor operations like `transpose, view, expand, narrow etc` do not change the original tensor, instead they modify the properties of the tensor. \n",
    "    - For example, `transpose`, demonstrated here, would change the shape (index), but both the old and modified property tensor share the same memory block with different indexes/addresses.\n",
    "    - This is why it's non-contiguous and we need to make it contiguous for some operations and typically for efficiency purpose.\n",
    "- This is not a blanket statement, but you typically want your tensors to be contiguous as it prevents additional overhead incurred from translating addresses.\n",
    "    - Whenever there's a warning that prompts you the tensor is not contiguous, just call `.contiguous()` and you typically should be good to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contiguous 10 by 5 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n",
      "  0   1   2   3   4\n",
      "  5   6   7   8   9\n",
      " 10  11  12  13  14\n",
      " 15  16  17  18  19\n",
      " 20  21  22  23  24\n",
      " 25  26  27  28  29\n",
      " 30  31  32  33  34\n",
      " 35  36  37  38  39\n",
      " 40  41  42  43  44\n",
      " 45  46  47  48  49\n"
     ]
    }
   ],
   "source": [
    "contiguous_tensor = torch.arange(50).view(10, 5)\n",
    "print(contiguous_tensor.shape)\n",
    "# Pretty print quick hack via Pandas DataFrame\n",
    "print(pd.DataFrame(contiguous_tensor.numpy()).to_string(index=False, header=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stride 5 by 1\n",
    "- Stride here shows how we need:\n",
    "    - 5 steps to move one row to the next &\n",
    "    - 1 step to move from one column to the next (contiguous)\n",
    "        - If this becomes anything other than 1 step, it becomes not contiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(contiguous_tensor.stride())\n",
    "print(contiguous_tensor.is_contiguous())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-contiguous Tensor via Transpose Operation\n",
    "- In order to access the next \"column\" value of 5, we have to take 5 steps despite the transpose as if we would do in our original tensor\n",
    "- Because the original tensor and this transposed tensor share the same memory block!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10])\n",
      " 0  5  10  15  20  25  30  35  40  45\n",
      " 1  6  11  16  21  26  31  36  41  46\n",
      " 2  7  12  17  22  27  32  37  42  47\n",
      " 3  8  13  18  23  28  33  38  43  48\n",
      " 4  9  14  19  24  29  34  39  44  49\n"
     ]
    }
   ],
   "source": [
    "non_contiguous_tensor = contiguous_tensor.t()\n",
    "print(non_contiguous_tensor.shape)\n",
    "print(pd.DataFrame(non_contiguous_tensor.numpy()).to_string(index=False, header=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stride 1 by 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5)\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(non_contiguous_tensor.stride())\n",
    "print(non_contiguous_tensor.is_contiguous())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Contiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Convert to contiguous\n",
    "convert_contiguous = non_contiguous_tensor.contiguous()\n",
    "print(convert_contiguous.is_contiguous())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable",
   "language": "python",
   "name": "stable"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
