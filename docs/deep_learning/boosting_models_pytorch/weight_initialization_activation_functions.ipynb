{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Weight Initializations & Activation Functions\n",
    "\n",
    "## Recap of Logistic Regression\n",
    "<img src=\"./images/cross_entropy_final_4.png\" alt=\"deeplearningwizard\" style=\"width: 900px;\"/>\n",
    "\n",
    "## Recap of Feedforward Neural Network Activation Function\n",
    "<img src=\"./images/logistic_regression_comparison_nn5.png\" alt=\"deeplearningwizard\" style=\"width: 900px;\"/>\n",
    "\n",
    "#### Sigmoid (Logistic)\n",
    "- $\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "- Input number $\\rightarrow$ [0, 1]\n",
    "    - Large negative number $\\rightarrow$ 0\n",
    "    - Large positive number $\\rightarrow$ 1\n",
    "- Cons: \n",
    "    1. Activation saturates at 0 or 1 with **gradients $\\approx$ 0**\n",
    "        - No signal to update weights $\\rightarrow$ **cannot learn**\n",
    "        - Solution: Have to carefully initialize weights to prevent this\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f66b07db6a0>]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    a = []\n",
    "    for item in x:\n",
    "        a.append(1/(1+np.exp(-item)))\n",
    "    return a\n",
    "\n",
    "x = np.arange(-10., 10., 0.2)\n",
    "sig = sigmoid(x)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.plot(x,sig, linewidth=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tanh\n",
    "- $\\tanh(x) = 2 \\sigma(2x) -1$\n",
    "    - A scaled sigmoid function\n",
    "- Input number $\\rightarrow$ [-1, 1]\n",
    "- Cons: \n",
    "    1. Activation saturates at 0 or 1 with **gradients $\\approx$ 0**\n",
    "        - No signal to update weights $\\rightarrow$ **cannot learn**\n",
    "        - **Solution**: Have to carefully initialize weights to prevent this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f66b05382e8>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(-10., 10., 0.2)\n",
    "tanh = np.dot(2, sigmoid(np.dot(2, x))) - 1\n",
    "\n",
    "plt.plot(x,tanh, linewidth=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLUs\n",
    "- $f(x) = \\max(0, x)$\n",
    "- Pros:\n",
    "    1. Accelerates convergence $\\rightarrow$ **train faster**\n",
    "    2. **Less computationally expensive operation** compared to Sigmoid/Tanh exponentials\n",
    "- Cons:\n",
    "    1. Many ReLU units \"die\" $\\rightarrow$ **gradients = 0** forever\n",
    "        - **Solution**: careful learning rate and weight initialization choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f66b04b8278>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(-10., 10., 0.2)\n",
    "relu = np.maximum(x, 0)\n",
    "\n",
    "plt.plot(x,relu, linewidth=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need weight initializations or new activation functions?\n",
    "- **To prevent vanishing/exploding gradients**\n",
    "\n",
    "### Case 1: Sigmoid/Tanh\n",
    "- **Problem**\n",
    "    - If variance of input too large: gradients = 0 (vanishing gradients)\n",
    "    - If variance of input too small: linear $\\rightarrow$ gradients = constant value\n",
    "- **Solutions**\n",
    "    - Want a constant variance of input to achieve non-linearity $\\rightarrow$ unique gradients for unique updates\n",
    "        - Xavier Initialization (good constant variance for Sigmoid/Tanh)\n",
    "        - ReLU or Leaky ReLU\n",
    "\n",
    "### Case 2: ReLU\n",
    "- **Solution to Case 1**\n",
    "    - Regardless of variance of input: gradients = 0 or 1 \n",
    "- **Problem**\n",
    "    - But those with 0: no updates (\"dead ReLU units\") \n",
    "    - Has unlimited output size with input > 0 (explodes gradients subsequently)\n",
    "- **Solutions**\n",
    "    - He Initialization (good constant variance)\n",
    "    - Leaky ReLU\n",
    "\n",
    "### Case 3: Leaky ReLU\n",
    "- **Solution to Case 2**\n",
    "    - Solves the 0 signal issue when input < 0 \n",
    "![](./images/leaky_relu_compare2.png)\n",
    "- **Problem**\n",
    "    - Has unlimited output size with input > 0 (explodes)\n",
    "- **Solution**\n",
    "    - He Initialization (good constant variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of weight initialization solutions to activations\n",
    "- Tanh/Sigmoid vanishing gradients can be solved with Xavier initialization\n",
    "    - Good range of constant variance\n",
    "- ReLU/Leaky ReLU exploding gradients can be solved with He initialization\n",
    "    - Good range of constant variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Types of weight intializations\n",
    "\n",
    "#### Zero Initialization: set all weights to 0 \n",
    "- Every neuron in the network computes the same output $\\rightarrow$ computes the same gradient $\\rightarrow$ same parameter updates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal Initialization: set all weights to random small numbers\n",
    "- Every neuron in the network computes different output $\\rightarrow$ computes different gradient $\\rightarrow$ different parameter updates \n",
    "- \"Symmetry breaking\" \n",
    "- Problem: variance that grows with the number of inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lecun Initialization: normalize variance\n",
    "- Solves growing variance with the number of inputs $\\rightarrow$ constant variance \n",
    "- Look at a simple feedforward neural network\n",
    "![](./images/nn2.png)\n",
    "\n",
    "**Some equations to explain**\n",
    "- $Y = AX + B$\n",
    "- $y = a_1x_1 + a_2x_2 + \\cdot + a_n x_n + b$\n",
    "- $Var(y) = Var(a_1x_1 + a_2x_2 + \\cdot + a_n x_n + b)$\n",
    "- $Var(a_i x_i) = E(x_i)^2 Var(a_i) + E(a_i)^2Var(x_i) + Var(a_i)Var(x_i)$\n",
    "    - General term, you might be more familiar with the following\n",
    "        - $Var(XY) = E(X)^2 Var(Y) + E(Y)^2Var(X) + Var(X)Var(Y)$\n",
    "    - $E(x_i)$: expectation/mean of $x_i$\n",
    "    - $E(a_i)$: expectation/mean of $a_i$\n",
    "- Assuming inputs/weights drawn i.i.d. with Gaussian distribution of mean=0\n",
    "    - $E(x_i) = E(a_i) = 0$\n",
    "    - $Var(a_i x_i) = Var(a_i)Var(x_i)$\n",
    "- $Var(y) = Var(a_1)Var(x_1) + \\cdot + Var(a_n)Var(x_n) $\n",
    "    - Since the bias, b, is a constant, $Var(b) = 0$\n",
    "- Since i.i.d.\n",
    "    - $Var(y) = n \\times Var(a_i)Var(x_i) $\n",
    "- Since we want constant variance where $ Var(y) = Var(x_i) $\n",
    "    - $1 = nVar(a_i)$\n",
    "    - $Var(a_i) = \\frac{1}{n}$\n",
    "- This is essentially [Lecun initialization, from his paper titled \"Efficient Backpropagation\"](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)\n",
    "    - We draw our weights i.i.d. with mean=0 and variance = $\\frac{1}{n}$\n",
    "    - Where $n$ is the number of **input units** in the weight tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improvements to Lecun Intialization\n",
    "- They are essentially slight modifications to Lecun'98 initialization\n",
    "- Xavier Intialization\n",
    "    - Works better for layers with Sigmoid activations \n",
    "    - $var(a_i) = \\frac{1}{n_{in} + n_{out}}$\n",
    "        - Where $n_{in}$ and $n_{out}$ are the number of input and output units in the weight tensor respectively\n",
    "- Kaiming Initialization\n",
    "    - Works better for layers with ReLU or LeakyReLU activations \n",
    "    - $var(a_i) = \\frac{2}{n_{in}}$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of  weight initializations\n",
    "- Normal Distribution\n",
    "- Lecun Normal Distribution\n",
    "- Xavier (Glorot) Normal Distribution \n",
    "- Kaiming (He) Normal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initializations with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Initialization: Tanh Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 LR: [0.1]\n",
      "Iteration: 500. Loss: 0.5192779302597046. Accuracy: 87.9\n",
      "Epoch: 1 LR: [0.096]\n",
      "Iteration: 1000. Loss: 0.4060308337211609. Accuracy: 90.15\n",
      "Epoch: 2 LR: [0.09216]\n",
      "Iteration: 1500. Loss: 0.2880493104457855. Accuracy: 90.71\n",
      "Epoch: 3 LR: [0.08847359999999999]\n",
      "Iteration: 2000. Loss: 0.23173095285892487. Accuracy: 91.99\n",
      "Epoch: 4 LR: [0.084934656]\n",
      "Iteration: 2500. Loss: 0.23814399540424347. Accuracy: 92.32\n",
      "Iteration: 3000. Loss: 0.19513173401355743. Accuracy: 92.55\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Scheduler import\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Linear weight, W,  Y = WX + B\n",
    "        nn.init.normal_(self.fc1.weight, mean=0, std=1)\n",
    "        # Non-linearity\n",
    "        self.tanh = nn.Tanh()\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "        nn.init.normal_(self.fc2.weight, mean=0, std=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Linear function\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity\n",
    "        out = self.tanh(out)\n",
    "        # Linear function (readout)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "'''\n",
    "STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS\n",
    "'''\n",
    "# step_size: at how many multiples of epoch you decay\n",
    "# step_size = 1, after every 2 epoch, new_lr = lr*gamma \n",
    "# step_size = 2, after every 2 epoch, new_lr = lr*gamma \n",
    "\n",
    "# gamma = decaying factor\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.96)\n",
    "\n",
    "'''\n",
    "STEP 8: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    # Decay Learning Rate\n",
    "    scheduler.step()\n",
    "    # Print Learning Rate\n",
    "    print('Epoch:', epoch,'LR:', scheduler.get_lr())\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as tensors with gradient accumulation abilities\n",
    "        images = images.view(-1, 28*28).requires_grad_()\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to a Torch Variable\n",
    "                images = images.view(-1, 28*28)\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "               # Total correct predictions\n",
    "                correct += (predicted.type(torch.FloatTensor).cpu() == labels.type(torch.FloatTensor)).sum()\n",
    "\n",
    "            accuracy = 100. * correct.item() / total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecun Initialization: Tanh Activation\n",
    "- By default, PyTorch uses Lecun initialization, so nothing new has to be done here compared to using Normal, Xavier or Kaiming initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 LR: [0.1]\n",
      "Iteration: 500. Loss: 0.20123475790023804. Accuracy: 95.63\n",
      "Epoch: 1 LR: [0.096]\n",
      "Iteration: 1000. Loss: 0.10885068774223328. Accuracy: 96.48\n",
      "Epoch: 2 LR: [0.09216]\n",
      "Iteration: 1500. Loss: 0.1296212077140808. Accuracy: 97.22\n",
      "Epoch: 3 LR: [0.08847359999999999]\n",
      "Iteration: 2000. Loss: 0.05178885534405708. Accuracy: 97.36\n",
      "Epoch: 4 LR: [0.084934656]\n",
      "Iteration: 2500. Loss: 0.02619408629834652. Accuracy: 97.61\n",
      "Iteration: 3000. Loss: 0.02096685953438282. Accuracy: 97.7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Scheduler import\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Non-linearity\n",
    "        self.tanh = nn.Tanh()\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Linear function\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity\n",
    "        out = self.tanh(out)\n",
    "        # Linear function (readout)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "'''\n",
    "STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS\n",
    "'''\n",
    "# step_size: at how many multiples of epoch you decay\n",
    "# step_size = 1, after every 2 epoch, new_lr = lr*gamma \n",
    "# step_size = 2, after every 2 epoch, new_lr = lr*gamma \n",
    "\n",
    "# gamma = decaying factor\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.96)\n",
    "\n",
    "'''\n",
    "STEP 8: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    # Decay Learning Rate\n",
    "    scheduler.step()\n",
    "    # Print Learning Rate\n",
    "    print('Epoch:', epoch,'LR:', scheduler.get_lr())\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as tensors with gradient accumulation abilities\n",
    "        images = images.view(-1, 28*28).requires_grad_()\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to a Torch Variable\n",
    "                images = images.view(-1, 28*28)\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted.type(torch.FloatTensor).cpu() == labels.type(torch.FloatTensor)).sum()\n",
    "\n",
    "            accuracy = 100. * correct.item() / total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier Initialization: Tanh Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 LR: [0.1]\n",
      "Iteration: 500. Loss: 0.14800140261650085. Accuracy: 95.43\n",
      "Epoch: 1 LR: [0.096]\n",
      "Iteration: 1000. Loss: 0.17138008773326874. Accuracy: 96.58\n",
      "Epoch: 2 LR: [0.09216]\n",
      "Iteration: 1500. Loss: 0.07987994700670242. Accuracy: 96.95\n",
      "Epoch: 3 LR: [0.08847359999999999]\n",
      "Iteration: 2000. Loss: 0.07756654918193817. Accuracy: 97.23\n",
      "Epoch: 4 LR: [0.084934656]\n",
      "Iteration: 2500. Loss: 0.05563584715127945. Accuracy: 97.6\n",
      "Iteration: 3000. Loss: 0.07122127711772919. Accuracy: 97.49\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Scheduler import\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Linear weight, W,  Y = WX + B\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        # Non-linearity\n",
    "        self.tanh = nn.Tanh()\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Linear function\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity\n",
    "        out = self.tanh(out)\n",
    "        # Linear function (readout)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "'''\n",
    "STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS\n",
    "'''\n",
    "# step_size: at how many multiples of epoch you decay\n",
    "# step_size = 1, after every 2 epoch, new_lr = lr*gamma \n",
    "# step_size = 2, after every 2 epoch, new_lr = lr*gamma \n",
    "\n",
    "# gamma = decaying factor\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.96)\n",
    "\n",
    "'''\n",
    "STEP 8: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    # Decay Learning Rate\n",
    "    scheduler.step()\n",
    "    # Print Learning Rate\n",
    "    print('Epoch:', epoch,'LR:', scheduler.get_lr())\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as tensors with gradient accumulation abilities\n",
    "        images = images.view(-1, 28*28).requires_grad_()\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to a Torch Variable\n",
    "                images = images.view(-1, 28*28)\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted.type(torch.FloatTensor).cpu() == labels.type(torch.FloatTensor)).sum()\n",
    "\n",
    "            accuracy = 100. * correct.item() / total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier Initialization: ReLU Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 LR: [0.1]\n",
      "Iteration: 500. Loss: 0.1245984435081482. Accuracy: 95.82\n",
      "Epoch: 1 LR: [0.096]\n",
      "Iteration: 1000. Loss: 0.14348150789737701. Accuracy: 96.72\n",
      "Epoch: 2 LR: [0.09216]\n",
      "Iteration: 1500. Loss: 0.10421314090490341. Accuracy: 97.3\n",
      "Epoch: 3 LR: [0.08847359999999999]\n",
      "Iteration: 2000. Loss: 0.04693891853094101. Accuracy: 97.29\n",
      "Epoch: 4 LR: [0.084934656]\n",
      "Iteration: 2500. Loss: 0.06869587302207947. Accuracy: 97.61\n",
      "Iteration: 3000. Loss: 0.056865859776735306. Accuracy: 97.48\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Scheduler import\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Linear weight, W,  Y = WX + B\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        # Non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Linear function\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity\n",
    "        out = self.relu(out)\n",
    "        # Linear function (readout)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "'''\n",
    "STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS\n",
    "'''\n",
    "# step_size: at how many multiples of epoch you decay\n",
    "# step_size = 1, after every 2 epoch, new_lr = lr*gamma \n",
    "# step_size = 2, after every 2 epoch, new_lr = lr*gamma \n",
    "\n",
    "# gamma = decaying factor\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.96)\n",
    "\n",
    "'''\n",
    "STEP 8: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    # Decay Learning Rate\n",
    "    scheduler.step()\n",
    "    # Print Learning Rate\n",
    "    print('Epoch:', epoch,'LR:', scheduler.get_lr())\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as tensors with gradient accumulation abilities\n",
    "        images = images.view(-1, 28*28).requires_grad_()\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to a Torch Variable\n",
    "                images = images.view(-1, 28*28)\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted.type(torch.FloatTensor).cpu() == labels.type(torch.FloatTensor)).sum()\n",
    "\n",
    "            accuracy = 100. * correct.item() / total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## He Initialization: ReLU Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 LR: [0.1]\n",
      "Iteration: 500. Loss: 0.11658752709627151. Accuracy: 95.7\n",
      "Epoch: 1 LR: [0.096]\n",
      "Iteration: 1000. Loss: 0.15525035560131073. Accuracy: 96.65\n",
      "Epoch: 2 LR: [0.09216]\n",
      "Iteration: 1500. Loss: 0.09970294684171677. Accuracy: 97.07\n",
      "Epoch: 3 LR: [0.08847359999999999]\n",
      "Iteration: 2000. Loss: 0.04063304886221886. Accuracy: 97.23\n",
      "Epoch: 4 LR: [0.084934656]\n",
      "Iteration: 2500. Loss: 0.0719323456287384. Accuracy: 97.7\n",
      "Iteration: 3000. Loss: 0.04470040276646614. Accuracy: 97.39\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Scheduler import\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Linear weight, W,  Y = WX + B\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        # Non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Linear function\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity\n",
    "        out = self.relu(out)\n",
    "        # Linear function (readout)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "'''\n",
    "STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS\n",
    "'''\n",
    "# step_size: at how many multiples of epoch you decay\n",
    "# step_size = 1, after every 2 epoch, new_lr = lr*gamma \n",
    "# step_size = 2, after every 2 epoch, new_lr = lr*gamma \n",
    "\n",
    "# gamma = decaying factor\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.96)\n",
    "\n",
    "'''\n",
    "STEP 8: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    # Decay Learning Rate\n",
    "    scheduler.step()\n",
    "    # Print Learning Rate\n",
    "    print('Epoch:', epoch,'LR:', scheduler.get_lr())\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as tensors with gradient accumulation abilities\n",
    "        images = images.view(-1, 28*28).requires_grad_()\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to a Torch Variable\n",
    "                images = images.view(-1, 28*28)\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted.type(torch.FloatTensor).cpu() == labels.type(torch.FloatTensor)).sum()\n",
    "\n",
    "            accuracy = 100. * correct.item() / total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization Performance\n",
    "\n",
    "| Initialization: Activation        | Test Accuracy           | \n",
    "| :-------------: |:-------------:| \n",
    "| Normal: Tanh| 92.55 | \n",
    "| Lecun: Tanh| 97.7 | \n",
    "| Xavier: Tanh    | 97.49     |\n",
    "| Xavier: ReLU | 97.48    |  \n",
    "| He: ReLU | 97.39     | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "- Recap of LG\n",
    "- Recap of FNN\n",
    "- Recap of Activation Functions\n",
    "    - Sigmoid (Logistic)\n",
    "    - Tanh\n",
    "    - ReLU\n",
    "- Need for Weight Initializations\n",
    "    - Sigmoid/Tanh: vanishing gradients\n",
    "        - Constant Variance initialization with Xavier \n",
    "    - ReLU: exploding gradients with dead units\n",
    "        - He Initialization\n",
    "    - Leaky ReLU: exploding gradients only\n",
    "        - He Initialization\n",
    "- Types of weight initialisations\n",
    "    - Zero\n",
    "    - Normal: growing weight variance\n",
    "    - Lecun: constant variance\n",
    "    - Xavier: constant variance for Sigmoid/Tanh\n",
    "    - Kaiming He: constant variance for ReLU activations\n",
    "- PyTorch implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latest",
   "language": "python",
   "name": "latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
