{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Equation\n",
    "- This is the simplified equation we have been using on how we update our parameters to reach good values (good local or global minima)\n",
    "- $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta$\n",
    "    - $\\theta$: parameters (our tensors with gradient accumulation abilities)\n",
    "    - $\\eta$: learning rate (how fast we want to learn)\n",
    "    - $\\nabla_\\theta$: gradients of loss with respect to the model's parameters\n",
    "- Even simplier equation in English: `parameters = parameters - learning_rate * parameters_gradients`\n",
    " \n",
    "## Simplified Equation Breakdown\n",
    "- Our simplified equation can be broken down into 2 parts\n",
    "    1. Backpropagation: getting our gradients\n",
    "        - Our partial derivatives of loss (scalar number) with respect to (w.r.t.) our model's parameters and w.r.t. our input\n",
    "        - Backpropagation gets us $\\nabla_\\theta$ which is our gradient\n",
    "    2. Gradient descent: using our gradients to update our parameters\n",
    "        - Somehow, the terms backpropagation and gradient descent are often mixed together. But they're totally different. \n",
    "        - Gradient descent relates to using our gradients obtained from backpropagation to update our weights.\n",
    "        - Gradient descent: $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta$\n",
    "\n",
    "## Steps\n",
    "- Derivatives\n",
    "- Partial Derivatives\n",
    "- Gradient\n",
    "- Gradient, Jacobian and Generalized Jacobian Differences\n",
    "- Backpropagation: computing gradient without\n",
    "- Gradient descent: using gradient to update parameters\n",
    "\n",
    "\n",
    "### Derivative\n",
    "- Given a simple cubic equation: $f(x) = 2x^3 + 5$\n",
    "- Calculating the derivative $\\frac{df(x)}{dx}$ is simply calculating the difference in values of $y$ for an extremely small (infinitesimally) change in value of $x$ which is frequently labelled as $h$\n",
    "    - $\\frac{df(x)}{dx} = \\displaystyle{\\lim_{h \\to 0}} \\frac{f(x + h) - f(x)}{h}$\n",
    "        - $\\frac{f(x + h) - f(x)}{h}$ is the slope formula similar to what you may be familiar with:\n",
    "            - Change in $y$ over change in $x$: $\\frac{\\Delta y}{\\Delta x}$\n",
    "        - And the derivative is the slope when $h \\rightarrow 0$, in essence a super teeny small $h$\n",
    "        - ![](./images/grad.png)\n",
    "- Let's break down $\\frac{df}{dx} = \\displaystyle{\\lim_{h \\to 0}} \\frac{f(x + h) - f(x)}{h}$\n",
    "    - $\\displaystyle{\\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}}$\n",
    "    - $\\displaystyle{\\lim_{h \\to 0} \\frac{(2(x+h)^3 + 5) - 2x^3 + 5}{h}}$\n",
    "    - $\\displaystyle{\\lim_{h \\to 0} \\frac{2(x^2 + 2xh + h^2)(x+h) - 2x^3}{h}}$\n",
    "    - $\\displaystyle{\\lim_{h \\to 0} \\frac{2(x^3 + 2x^2h + h^3 + x^2h + 2xh^2 + h^3) - 2x^3}{h}}$\n",
    "    - $\\displaystyle{\\lim_{h \\to 0}\\frac{2(x^3 + 3x^2h + h^3 + 2xh^2) - 2x^3}{h}}$\n",
    "    - $\\displaystyle{\\lim_{h \\to 0} \\frac{6x^2h + h^3 + 2xh^2}{h}}$\n",
    "    - $\\displaystyle{\\lim_{h \\to 0} 6x^2 + h^2 + 2xh} = 6x^2$\n",
    "\n",
    "### Partial Derivative \n",
    "- Ok, it's simple to calculate our derivative when we've only one variable in our function. \n",
    "- If we've more than one (as with our parameters in our models), we need to calculate our partial derivatives of our function with respect to our variables\n",
    "- Given a simple equation $f(x, z) = 4x^4z^3$, let us get our partial derivatives\n",
    "- 2 parts: partial derivative of our function w.r.t. x and z\n",
    "    1. Partial derivative of our function w.r.t. x: $\\frac{\\delta f(x, z)}{\\delta x}$\n",
    "        - Let $z$ term be a constant, $a$ \n",
    "        - $f(x, z) = 4x^4a$\n",
    "        -  $\\frac{\\delta f(x, z)}{\\delta x} = 16x^3a$\n",
    "        - Now we substitute $a$ with our z term, $a = z^3$\n",
    "        - $\\frac{\\delta f(x, z)}{\\delta x} = 16x^3z^3$\n",
    "    2. Partial derivative of our function w.r.t. z: $\\frac{\\delta f(x, z)}{\\delta z}$\n",
    "        - Let $x$ term be a constant, $a$\n",
    "        - $f(x, z) = 4az^3$\n",
    "        - $\\frac{\\delta f(x, z)}{\\delta z} = 12az^2$\n",
    "        - Now we substitute $a$ with our $x$ term, $a = x^4$\n",
    "        - $\\frac{\\delta f(x, z)}{\\delta z} = 12x^4z^2$\n",
    "- Ta da! We made it, we calculated our partial derivatives of our function w.r.t. the different variables\n",
    "\n",
    "### Gradient\n",
    "- We can now put all our partial derivatives into a vector of partial derivatives\n",
    "    - Also called \"gradient\"\n",
    "    - Represented by $\\nabla_{(x,z)}$\n",
    "- $\\nabla_{(x,z)} = \\begin{bmatrix} \\frac{df(x,z)}{dx} \\\\ \\frac{df(x,z)}{dz} \\end{bmatrix} = \\begin{bmatrix} 16x^3z^3 \\\\ 12x^4z^2 \\end{bmatrix}$\n",
    "- It is critical to note that the term gradient applies for $f : \\mathbb{R}^N \\rightarrow \\mathbb{R}$\n",
    "    - Where our function maps a vector input to a scalar output: in deep learning, our loss function that produces a scalar loss\n",
    "    \n",
    "### Gradient, Jacobian, and Generalized Jacobian\n",
    "In the case where we have non-scalar outputs, these are the right terms of matrices or vectors containing our partial derivatives\n",
    "1. Gradient: vector input to scalar output\n",
    "    - $f : \\mathbb{R}^N \\rightarrow \\mathbb{R}$\n",
    "2. Jacobian: vector input to vector output\n",
    "    - $f : \\mathbb{R}^N \\rightarrow \\mathbb{R}^M$\n",
    "3. Generalized Jacobian: tensor input to tensor output\n",
    "    - In this case, a tensor can be any number of dimensions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
